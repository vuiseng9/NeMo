[90mmodel.config.tensor_model_parallel_size: int = <[unset; default: 1]>[0m
[90mmodel.config.pipeline_model_parallel_comm_backend: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.pipeline_model_parallel_size: int = <[unset; default: 1]>[0m
[90mmodel.config.virtual_pipeline_model_parallel_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.sequence_parallel: bool = <[unset; default: False]>[0m
[90mmodel.config.context_parallel_size: int = <[unset; default: 1]>[0m
[90mmodel.config.hierarchical_context_parallel_sizes: typing.Optional[list[int]] = <[unset; default: None]>[0m
[90mmodel.config.expert_model_parallel_size: int = <[unset; default: 1]>[0m
[90mmodel.config.expert_tensor_parallel_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_extended_tp: bool = <[unset; default: False]>[0m
[90mmodel.config.perform_initialization: bool = <[unset; default: True]>[0m
[90mmodel.config.use_cpu_initialization: bool = <[unset; default: False]>[0m
[90mmodel.config.fp16: bool = <[unset; default: False]>[0m
[90mmodel.config.bf16: bool = <[unset; default: False]>[0m
[90mmodel.config.params_dtype: dtype = <[unset; default: torch.float32]>[0m
[90mmodel.config.timers: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.finalize_model_grads_func: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.grad_scale_func: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.no_sync_func: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.grad_sync_func: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.param_sync_func: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.deterministic_mode: bool = <[unset; default: False]>[0m
[90mmodel.config.enable_autocast: bool = <[unset; default: False]>[0m
[90mmodel.config.autocast_dtype: typing.Optional[torch.dtype] = <[unset; default: None]>[0m
[90mmodel.config.num_microbatches_with_partial_activation_checkpoints: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.gradient_accumulation_fusion: bool = <[unset; default: True]>[0m
[90mmodel.config.async_tensor_model_parallel_allreduce: bool = <[unset; default: False]>[0m
[90mmodel.config.use_te_rng_tracker: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_overlap: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_bulk_wgrad: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_bulk_dgrad: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_overlap_ag: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_overlap_rs: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_overlap_rs_dgrad: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_split_ag: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_atomic_ag: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_split_rs: bool = <[unset; default: True]>[0m
[90mmodel.config.tp_comm_atomic_rs: bool = <[unset; default: False]>[0m
model.config.cross_entropy_loss_fusion: bool = False
[90mmodel.config.cross_entropy_fusion_impl: str = <[unset; default: 'native']>[0m
[90mmodel.config.tp_comm_overlap_disable_qkv: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_overlap_disable_fc1: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_comm_bootstrap_backend: str = <[unset; default: 'nccl']>[0m
[90mmodel.config.pipeline_dtype: dtype = <[unset; default: None]>[0m
[90mmodel.config.variable_seq_lengths: bool = <[unset; default: False]>[0m
[90mmodel.config.overlap_p2p_comm: bool = <[unset; default: False]>[0m
[90mmodel.config.batch_p2p_comm: bool = <[unset; default: True]>[0m
[90mmodel.config.batch_p2p_sync: bool = <[unset; default: True]>[0m
[90mmodel.config.use_ring_exchange_p2p: bool = <[unset; default: False]>[0m
[90mmodel.config.deallocate_pipeline_outputs: bool = <[unset; default: True]>[0m
[90mmodel.config.defer_embedding_wgrad_compute: bool = <[unset; default: False]>[0m
[90mmodel.config.wgrad_deferral_limit: int = <[unset; default: 0]>[0m
[90mmodel.config.pipeline_model_parallel_split_rank: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.overlap_p2p_comm_warmup_flush: bool = <[unset; default: False]>[0m
[90mmodel.config.microbatch_group_size_per_vp_stage: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.delay_wgrad_compute: bool = <[unset; default: False]>[0m
[90mmodel.config.cpu_offloading: bool = <[unset; default: False]>[0m
[90mmodel.config.cpu_offloading_num_layers: int = <[unset; default: 0]>[0m
[90mmodel.config._cpu_offloading_context: typing.Optional[typing.ContextManager] = <[unset; default: None]>[0m
[90mmodel.config.cpu_offloading_activations: bool = <[unset; default: True]>[0m
[90mmodel.config.cpu_offloading_weights: bool = <[unset; default: True]>[0m
[90mmodel.config.barrier_with_L1_time: bool = <[unset; default: True]>[0m
[90mmodel.config.num_layers: int = <[unset; default: 32]>[0m
[90mmodel.config.mtp_num_layers: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.mtp_loss_scaling_factor: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.num_layers_in_first_pipeline_stage: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.num_layers_in_last_pipeline_stage: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.pipeline_model_parallel_layout: typing.Union[str, list, megatron.core.transformer.pipeline_parallel_layer_layout.PipelineParallelLayerLayout, NoneType] = <[unset; default: None]>[0m
[90mmodel.config.account_for_embedding_in_pipeline_split: bool = <[unset; default: False]>[0m
[90mmodel.config.account_for_loss_in_pipeline_split: bool = <[unset; default: False]>[0m
[90mmodel.config.hidden_size: int = <[unset; default: 4096]>[0m
[90mmodel.config.num_attention_heads: int = <[unset; default: 32]>[0m
[90mmodel.config.attention_backend: AttnBackend = <[unset; default: <AttnBackend.auto: 5>]>[0m
[90mmodel.config.softmax_scale: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.num_query_groups: int = <[unset; default: 8]>[0m
[90mmodel.config.ffn_hidden_size: int = <[unset; default: 14336]>[0m
[90mmodel.config.kv_channels: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.hidden_dropout: float = <[unset; default: 0.0]>[0m
[90mmodel.config.attention_dropout: float = <[unset; default: 0.0]>[0m
[90mmodel.config.fp32_residual_connection: bool = <[unset; default: False]>[0m
[90mmodel.config.apply_residual_connection_post_layernorm: bool = <[unset; default: False]>[0m
[90mmodel.config.layernorm_epsilon: float = <[unset; default: 1e-05]>[0m
[90mmodel.config.layernorm_zero_centered_gamma: bool = <[unset; default: False]>[0m
[90mmodel.config.add_bias_linear: bool = <[unset; default: False]>[0m
[90mmodel.config.add_qkv_bias: bool = <[unset; default: False]>[0m
[90mmodel.config.gated_linear_unit: bool = <[unset; default: True]>[0m
[90mmodel.config.activation_func: typing.Callable = <[unset; default: <function silu at 0x7e7ccc71c220>]>[0m
[90mmodel.config.activation_func_fp8_input_store: bool = <[unset; default: False]>[0m
[90mmodel.config.num_moe_experts: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.rotary_interleaved: bool = <[unset; default: False]>[0m
[90mmodel.config.window_size: typing.Optional[typing.Tuple[int, int]] = <[unset; default: None]>[0m
[90mmodel.config.normalization: str = <[unset; default: 'RMSNorm']>[0m
[90mmodel.config.qk_layernorm: bool = <[unset; default: False]>[0m
[90mmodel.config.test_mode: bool = <[unset; default: False]>[0m
[90mmodel.config.calculate_per_token_loss: bool = <[unset; default: False]>[0m
[90mmodel.config.multi_latent_attention: bool = <[unset; default: False]>[0m
[90mmodel.config.no_rope_freq: typing.Union[int, typing.List[int], NoneType] = <[unset; default: None]>[0m
[90mmodel.config.moe_deepep_num_sms: int = <[unset; default: 20]>[0m
[90mmodel.config.init_method: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.output_layer_init_method: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90mmodel.config.init_method_std: float = <[unset; default: 0.01]>[0m
[90mmodel.config.init_model_with_meta_device: bool = <[unset; default: False]>[0m
[90mmodel.config.apply_query_key_layer_scaling: bool = <[unset; default: False]>[0m
[90mmodel.config.attention_softmax_in_fp32: bool = <[unset; default: False]>[0m
[90mmodel.config.disable_bf16_reduced_precision_matmul: bool = <[unset; default: False]>[0m
[90mmodel.config.bias_activation_fusion: bool = <[unset; default: True]>[0m
[90mmodel.config.masked_softmax_fusion: bool = <[unset; default: True]>[0m
[90mmodel.config.persist_layer_norm: bool = <[unset; default: True]>[0m
[90mmodel.config.memory_efficient_layer_norm: bool = <[unset; default: False]>[0m
[90mmodel.config.bias_dropout_fusion: bool = <[unset; default: True]>[0m
[90mmodel.config.apply_rope_fusion: bool = <[unset; default: True]>[0m
[90mmodel.config.recompute_granularity: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.recompute_method: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.recompute_num_layers: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.distribute_saved_activations: typing.Optional[bool] = <[unset; default: None]>[0m
[90mmodel.config.recompute_modules: typing.Optional[typing.List[str]] = <[unset; default: None]>[0m
[90mmodel.config.fp8: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.fp8_recipe: typing.Optional[str] = <[unset; default: 'delayed']>[0m
[90mmodel.config.fp8_param: bool = <[unset; default: False]>[0m
[90mmodel.config.fp8_margin: int = <[unset; default: 0]>[0m
[90mmodel.config.fp8_interval: int = <[unset; default: 1]>[0m
[90mmodel.config.fp8_amax_history_len: int = <[unset; default: 1]>[0m
[90mmodel.config.fp8_amax_compute_algo: str = <[unset; default: 'most_recent']>[0m
[90mmodel.config.fp8_wgrad: bool = <[unset; default: True]>[0m
[90mmodel.config.fp8_dot_product_attention: bool = <[unset; default: False]>[0m
[90mmodel.config.fp8_multi_head_attention: bool = <[unset; default: False]>[0m
[90mmodel.config.tp_only_amax_red: bool = <[unset; default: False]>[0m
[90mmodel.config.first_last_layers_bf16: bool = <[unset; default: False]>[0m
[90mmodel.config.num_layers_at_start_in_bf16: int = <[unset; default: 1]>[0m
[90mmodel.config.num_layers_at_end_in_bf16: int = <[unset; default: 1]>[0m
[90mmodel.config.use_kitchen: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_shared_expert_intermediate_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_shared_expert_overlap: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_layer_freq: typing.Union[int, typing.List[int]] = <[unset; default: 1]>[0m
[90mmodel.config.moe_ffn_hidden_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_load_balancing_type: str = <[unset; default: 'aux_loss']>[0m
[90mmodel.config.moe_router_topk: int = <[unset; default: 2]>[0m
[90mmodel.config.moe_router_topk_limited_devices: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_padding_for_fp8: typing.Optional[bool] = <[unset; default: False]>[0m
[90mmodel.config.moe_router_num_groups: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_group_topk: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_pre_softmax: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_router_topk_scaling_factor: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_score_function: str = <[unset; default: 'softmax']>[0m
[90mmodel.config.moe_router_dtype: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.moe_router_enable_expert_bias: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_router_bias_update_rate: float = <[unset; default: 0.001]>[0m
[90mmodel.config.moe_router_force_load_balancing: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_grouped_gemm: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_use_legacy_grouped_gemm: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_aux_loss_coeff: float = <[unset; default: 0]>[0m
[90mmodel.config.moe_z_loss_coeff: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.moe_input_jitter_eps: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.moe_token_dropping: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_token_dispatcher_type: str = <[unset; default: 'allgather']>[0m
[90mmodel.config.moe_enable_deepep: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_per_layer_logging: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_expert_capacity_factor: typing.Optional[float] = <[unset; default: None]>[0m
[90mmodel.config.moe_pad_expert_input_to_capacity: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_token_drop_policy: str = <[unset; default: 'probs']>[0m
[90mmodel.config.moe_layer_recompute: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_permute_fusion: bool = <[unset; default: False]>[0m
[90mmodel.config.moe_apply_probs_on_input: bool = <[unset; default: False]>[0m
[90mmodel.config.cp_comm_type: typing.Union[str, typing.List[str], NoneType] = <[unset; default: None]>[0m
[90mmodel.config.enable_cuda_graph: bool = <[unset; default: False]>[0m
[90mmodel.config.cuda_graph_use_single_mempool: bool = <[unset; default: False]>[0m
[90mmodel.config.cuda_graph_retain_backward_graph: bool = <[unset; default: False]>[0m
[90mmodel.config.cuda_graph_warmup_steps: int = <[unset; default: 3]>[0m
[90mmodel.config.external_cuda_graph: bool = <[unset; default: False]>[0m
[90mmodel.config.cuda_graph_scope: str = <[unset; default: 'full']>[0m
[90mmodel.config.clone_scatter_output_in_embedding: bool = <[unset; default: True]>[0m
[90mmodel.config.disable_parameter_transpose_cache: bool = <[unset; default: False]>[0m
[90mmodel.config.config_logger_dir: str = <[unset; default: '']>[0m
[90mmodel.config.flash_decode: bool = <[unset; default: False]>[0m
[90mmodel.config.inference_rng_tracker: bool = <[unset; default: False]>[0m
[90mmodel.config.symmetric_ar_type: typing.Optional[str] = <[unset; default: None]>[0m
[90mmodel.config.mrope_section: typing.Optional[typing.List[int]] = <[unset; default: None]>[0m
[90mmodel.config.is_hybrid_model: bool = <[unset; default: False]>[0m
[90mmodel.config.mamba_state_dim: int = <[unset; default: 128]>[0m
[90mmodel.config.mamba_head_dim: int = <[unset; default: 64]>[0m
[90mmodel.config.mamba_num_groups: int = <[unset; default: 8]>[0m
[90mmodel.config.mamba_num_heads: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.use_mamba_mem_eff_path: bool = <[unset; default: True]>[0m
[90mmodel.config.mlp_chunks_for_prefill: int = <[unset; default: 1]>[0m
[90mmodel.config.heterogeneous_block_specs: bool = <[unset; default: False]>[0m
[90mmodel.config.hetereogenous_dist_checkpoint: bool = <[unset; default: False]>[0m
[90mmodel.config.quant_recipe: typing.Optional[megatron.core.quantization.quant_config.RecipeConfig] = <[unset; default: None]>[0m
[90mmodel.config.fp16_lm_cross_entropy: bool = <[unset; default: False]>[0m
[90mmodel.config.parallel_output: bool = <[unset; default: True]>[0m
[90mmodel.config.share_embeddings_and_output_weights: bool = <[unset; default: False]>[0m
[90mmodel.config.make_vocab_size_divisible_by: int = <[unset; default: 128]>[0m
[90mmodel.config.position_embedding_type: str = <[unset; default: 'rope']>[0m
[90mmodel.config.rotary_base: int = <[unset; default: 500000]>[0m
[90mmodel.config.rotary_percent: float = <[unset; default: 1.0]>[0m
[90mmodel.config.seq_len_interpolation_factor: typing.Optional[float] = <[unset; default: None]>[0m
model.config.seq_length: int = 2048
[90mmodel.config.scatter_embedding_sequence_parallel: bool = <[unset; default: True]>[0m
[90mmodel.config.use_transformer_engine_full_layer_spec: bool = <[unset; default: False]>[0m
[90mmodel.config.transformer_layer_spec: typing.Union[megatron.core.transformer.spec_utils.ModuleSpec, typing.Callable[[ForwardRef('GPTConfig')], megatron.core.transformer.spec_utils.ModuleSpec]] = <[unset; default: <function default_layer_spec at 0x7e7827588d60>]>[0m
[90mmodel.config.forward_step_fn: typing.Callable = <[unset; default: <function gpt_forward_step at 0x7e7827713920>]>[0m
[90mmodel.config.data_step_fn: typing.Callable = <[unset; default: <function gpt_data_step at 0x7e7827713740>]>[0m
[90mmodel.config.generation_config: typing.Optional[ForwardRef('GenerationConfig')] = <[unset; default: None]>[0m
[90mmodel.config.vocab_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mmodel.config.tp_comm_overlap_cfg: typing.Union[str, dict[str, typing.Any], NoneType] = <[unset; default: None]>[0m
[90mmodel.config.use_transformer_engine_op_fuser: typing.Optional[bool] = <[unset; default: None]>[0m
[90mmodel.optim: typing.Optional[nemo.lightning.pytorch.optim.base.OptimizerModule] = <[unset; default: None]>[0m
[90mmodel.tokenizer: typing.Optional[ForwardRef('TokenizerSpec')] = <[unset; default: None]>[0m
[90mmodel.model_transform: typing.Optional[typing.Callable[[torch.nn.modules.module.Module], torch.nn.modules.module.Module]] = <[unset; default: None]>[0m
[90mmodel.model_context_managers: typing.Optional[typing.List] = <[unset; default: []]>[0m
[90mdata.dataset_root: typing.Union[str, pathlib.Path, NoneType] = <[unset; default: None]>[0m
data.seq_length: int = 2048
[90mdata.tokenizer: typing.Optional[ForwardRef('TokenizerSpec')] = <[unset; default: None]>[0m
data.micro_batch_size: int = 1
data.global_batch_size: int = 128
[90mdata.rampup_batch_size: typing.Optional[typing.List[int]] = <[unset; default: None]>[0m
[90mdata.force_redownload: bool = <[unset; default: False]>[0m
[90mdata.delete_raw: bool = <[unset; default: True]>[0m
[90mdata.seed: int = <[unset; default: 1234]>[0m
[90mdata.memmap_workers: int = <[unset; default: 1]>[0m
[90mdata.num_workers: int = <[unset; default: 8]>[0m
[90mdata.pin_memory: bool = <[unset; default: True]>[0m
[90mdata.persistent_workers: bool = <[unset; default: False]>[0m
[90mdata.packed_sequence_specs: typing.Optional[ForwardRef('PackedSequenceSpecs')] = <[unset; default: None]>[0m
[90mdata.dataset_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = <[unset; default: None]>[0m
trainer.accelerator: typing.Union[str, lightning.pytorch.accelerators.accelerator.Accelerator] = 'gpu'
trainer.strategy.tensor_model_parallel_size: int = 1
trainer.strategy.pipeline_model_parallel_size: int = 1
[90mtrainer.strategy.pipeline_model_parallel_comm_backend: str = <[unset; default: None]>[0m
[90mtrainer.strategy.num_layers_in_first_pipeline_stage: typing.Optional[int] = <[unset; default: None]>[0m
[90mtrainer.strategy.num_layers_in_last_pipeline_stage: typing.Optional[int] = <[unset; default: None]>[0m
trainer.strategy.virtual_pipeline_model_parallel_size: typing.Optional[int] = None
[90mtrainer.strategy.microbatch_group_size_per_vp_stage: typing.Optional[int] = <[unset; default: None]>[0m
trainer.strategy.context_parallel_size: int = 1
trainer.strategy.sequence_parallel: bool = False
[90mtrainer.strategy.expert_model_parallel_size: int = <[unset; default: 1]>[0m
[90mtrainer.strategy.moe_extended_tp: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.expert_tensor_parallel_size: typing.Optional[int] = <[unset; default: None]>[0m
[90mtrainer.strategy.encoder_tensor_model_parallel_size: typing.Optional[int] = <[unset; default: 0]>[0m
[90mtrainer.strategy.encoder_pipeline_model_parallel_size: typing.Optional[int] = <[unset; default: 0]>[0m
[90mtrainer.strategy.account_for_embedding_in_pipeline_split: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.account_for_loss_in_pipeline_split: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.data_sampler: typing.Optional[ForwardRef('DataSampler')] = <[unset; default: None]>[0m
[90mtrainer.strategy.parallel_devices: typing.Optional[typing.List[torch.device]] = <[unset; default: None]>[0m
[90mtrainer.strategy.cluster_environment = <[unset; default: None]>[0m
[90mtrainer.strategy.checkpoint_io = <[unset; default: None]>[0m
[90mtrainer.strategy.find_unused_parameters: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.ckpt_load_optimizer: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ckpt_save_optimizer: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ddp: typing.Union[typing.Literal['megatron', 'pytorch'], megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig] = <[unset; default: 'megatron']>[0m
[90mtrainer.strategy.fsdp: typing.Optional[typing.Literal['megatron', 'pytorch']] = <[unset; default: None]>[0m
[90mtrainer.strategy.lazy_init: bool = <[unset; default: False]>[0m
trainer.strategy.pipeline_dtype: typing.Optional[torch.dtype] = torch.bfloat16
[90mtrainer.strategy.use_te_rng_tracker: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.use_sharp: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.save_ckpt_format: str = <[unset; default: 'torch_dist']>[0m
[90mtrainer.strategy.ckpt_async_save: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ckpt_torch_dist_multiproc: int = <[unset; default: None]>[0m
[90mtrainer.strategy.ckpt_assume_constant_structure: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.ckpt_parallel_save: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ckpt_parallel_save_within_dp: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.ckpt_parallel_load: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ckpt_parallel_save_optim: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.ckpt_load_directly_on_device: bool = <[unset; default: True]>[0m
trainer.strategy.ckpt_load_strictness: typing.Optional[ForwardRef('StrictHandling')] = 'log_all'
[90mtrainer.strategy.setup_optimizers: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.init_model_parallel: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.replace_progress_bar: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.progress_interval: int = <[unset; default: 1]>[0m
[90mtrainer.strategy.restore_config: typing.Optional[nemo.lightning.pytorch.strategies.utils.RestoreConfig] = <[unset; default: None]>[0m
[90mtrainer.strategy.megatron_log_level: int = <[unset; default: 0]>[0m
[90mtrainer.strategy.use_tp_pp_dp_mapping: bool = <[unset; default: False]>[0m
[90mtrainer.strategy.num_distributed_optimizer_instances: int = <[unset; default: 1]>[0m
[90mtrainer.strategy.nccl_communicator_config_path: typing.Optional[str] = <[unset; default: None]>[0m
[90mtrainer.strategy.use_gloo_process_groups: bool = <[unset; default: True]>[0m
[90mtrainer.strategy.pipeline_model_parallel_layout: typing.Union[str, typing.List[typing.List[str]], NoneType] = <[unset; default: None]>[0m
trainer.strategy.gradient_as_bucket_view = True
trainer.devices: typing.Union[list[int], str, int] = '1'
trainer.num_nodes: int = 1
[90mtrainer.precision: typing.Union[typing.Literal[64, 32, 16], typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], typing.Literal['64', '32', '16', 'bf16'], NoneType] = <[unset; default: None]>[0m
[90mtrainer.logger: typing.Union[lightning.pytorch.loggers.logger.Logger, collections.abc.Iterable[lightning.pytorch.loggers.logger.Logger], bool, NoneType] = <[unset; default: None]>[0m
[90mtrainer.callbacks[0].log_tokens_per_sec: bool = <[unset; default: False]>[0m
[90mtrainer.callbacks[0].timer_kwargs = <[unset; default: {}]>[0m
[90mtrainer.fast_dev_run: typing.Union[int, bool] = <[unset; default: False]>[0m
[90mtrainer.max_epochs: typing.Optional[int] = <[unset; default: None]>[0m
[90mtrainer.min_epochs: typing.Optional[int] = <[unset; default: None]>[0m
trainer.max_steps: int = 50
[90mtrainer.min_steps: typing.Optional[int] = <[unset; default: None]>[0m
[90mtrainer.max_time: typing.Union[str, datetime.timedelta, dict[str, int], NoneType] = <[unset; default: None]>[0m
[90mtrainer.limit_train_batches: typing.Union[int, float, NoneType] = <[unset; default: None]>[0m
trainer.limit_val_batches: typing.Union[int, float, NoneType] = None
trainer.limit_test_batches: typing.Union[int, float, NoneType] = None
[90mtrainer.limit_predict_batches: typing.Union[int, float, NoneType] = <[unset; default: None]>[0m
[90mtrainer.overfit_batches: typing.Union[int, float] = <[unset; default: 0.0]>[0m
trainer.val_check_interval: typing.Union[int, float, NoneType] = 10
[90mtrainer.check_val_every_n_epoch: typing.Optional[int] = <[unset; default: 1]>[0m
[90mtrainer.num_sanity_val_steps: typing.Optional[int] = <[unset; default: None]>[0m
trainer.log_every_n_steps: typing.Optional[int] = 1
[90mtrainer.enable_checkpointing: typing.Optional[bool] = <[unset; default: None]>[0m
[90mtrainer.enable_progress_bar: typing.Optional[bool] = <[unset; default: None]>[0m
[90mtrainer.enable_model_summary: typing.Optional[bool] = <[unset; default: None]>[0m
trainer.accumulate_grad_batches: int = 1
[90mtrainer.gradient_clip_val: typing.Union[int, float, NoneType] = <[unset; default: None]>[0m
[90mtrainer.gradient_clip_algorithm: typing.Optional[str] = <[unset; default: None]>[0m
[90mtrainer.deterministic: typing.Union[bool, typing.Literal['warn'], NoneType] = <[unset; default: None]>[0m
[90mtrainer.benchmark: typing.Optional[bool] = <[unset; default: None]>[0m
[90mtrainer.inference_mode: bool = <[unset; default: True]>[0m
trainer.use_distributed_sampler: bool = False
[90mtrainer.profiler: typing.Union[lightning.pytorch.profilers.profiler.Profiler, str, NoneType] = <[unset; default: None]>[0m
[90mtrainer.detect_anomaly: bool = <[unset; default: False]>[0m
[90mtrainer.barebones: bool = <[unset; default: False]>[0m
trainer.plugins.precision: typing.Literal['16-mixed', 'bf16-mixed', '32'] = 'bf16-mixed'
trainer.plugins.params_dtype: dtype = torch.bfloat16
trainer.plugins.pipeline_dtype: dtype = torch.bfloat16
[90mtrainer.plugins.autocast_dtype: dtype = <[unset; default: None]>[0m
trainer.plugins.autocast_enabled: bool = False
trainer.plugins.grad_reduce_in_fp32: bool = True
trainer.plugins.fp8: str = 'hybrid'
trainer.plugins.fp8_recipe: str = 'delayed'
[90mtrainer.plugins.first_last_layers_bf16: bool = <[unset; default: False]>[0m
trainer.plugins.fp8_margin: int = 0
trainer.plugins.fp8_amax_history_len: int = 1024
trainer.plugins.fp8_amax_compute_algo: str = 'max'
[90mtrainer.plugins.fp8_wgrad: bool = <[unset; default: True]>[0m
[90mtrainer.plugins.fp8_dot_product_attention: bool = <[unset; default: False]>[0m
[90mtrainer.plugins.fp8_multi_head_attention: bool = <[unset; default: False]>[0m
[90mtrainer.plugins.fp8_params: bool = <[unset; default: None]>[0m
trainer.plugins.fp8_param_gather: bool = True
[90mtrainer.plugins.fp16_loss_scale: float = <[unset; default: None]>[0m
[90mtrainer.plugins.fp16_initial_loss_scale: float = <[unset; default: 4294967296]>[0m
[90mtrainer.plugins.fp16_min_loss_scale: float = <[unset; default: 1.0]>[0m
[90mtrainer.plugins.fp16_loss_scale_window: int = <[unset; default: 1000]>[0m
[90mtrainer.plugins.fp16_hysteresis: int = <[unset; default: 2]>[0m
[90mtrainer.plugins.num_layers_at_start_in_bf16: int = <[unset; default: 0]>[0m
[90mtrainer.plugins.num_layers_at_end_in_bf16: int = <[unset; default: 0]>[0m
[90mtrainer.plugins.reuse_grad_buf_for_mxfp8_param_ag: bool = <[unset; default: False]>[0m
[90mtrainer.sync_batchnorm: bool = <[unset; default: False]>[0m
[90mtrainer.reload_dataloaders_every_n_epochs: int = <[unset; default: 0]>[0m
[90mtrainer.default_root_dir: typing.Union[str, pathlib.Path, NoneType] = <[unset; default: None]>[0m
[90mtrainer.enable_autolog_hparams: bool = <[unset; default: True]>[0m
[90mtrainer.model_registry: typing.Optional[str] = <[unset; default: None]>[0m
log.name: str = 'default'
log.log_dir: typing.Optional[str] = '/root/work/run/fri-ootb/ootb-h100x1-lora_squad'
[90mlog.explicit_log_dir: typing.Optional[str] = <[unset; default: None]>[0m
[90mlog.version: typing.Optional[str] = <[unset; default: None]>[0m
[90mlog.use_datetime_version: bool = <[unset; default: True]>[0m
[90mlog.log_local_rank_0_only: bool = <[unset; default: False]>[0m
[90mlog.log_global_rank_0_only: bool = <[unset; default: False]>[0m
[90mlog.files_to_copy: typing.Optional[typing.List[str]] = <[unset; default: None]>[0m
[90mlog.update_logger_directory: bool = <[unset; default: True]>[0m
[90mlog.ckpt.monitor: typing.Optional[str] = <[unset; default: 'val_loss']>[0m
[90mlog.ckpt.verbose: bool = <[unset; default: True]>[0m
log.ckpt.save_last: typing.Union[bool, typing.Literal['link'], NoneType] = 'link'
log.ckpt.save_top_k: int = 1
[90mlog.ckpt.save_weights_only: bool = <[unset; default: False]>[0m
[90mlog.ckpt.mode: str = <[unset; default: 'min']>[0m
[90mlog.ckpt.every_n_epochs: int = <[unset; default: None]>[0m
log.ckpt.every_n_train_steps: typing.Optional[int] = 10
[90mlog.ckpt.train_time_interval: typing.Optional[datetime.timedelta] = <[unset; default: None]>[0m
[90mlog.ckpt.save_on_train_epoch_end: typing.Optional[bool] = <[unset; default: False]>[0m
[90mlog.ckpt.save_optim_on_train_end: typing.Optional[bool] = <[unset; default: False]>[0m
[90mlog.ckpt.always_save_context: bool = <[unset; default: True]>[0m
[90mlog.ckpt.save_context_on_train_end: bool = <[unset; default: True]>[0m
log.ckpt.filename = '{model_name}--{val_loss:.2f}-{step}-{consumed_samples}'
log.tensorboard: typing.Optional[lightning.pytorch.loggers.tensorboard.TensorBoardLogger] = None
log.wandb.name: typing.Optional[str] = 'ootb-h100x1-lora_squad'
[90mlog.wandb.save_dir: typing.Union[str, pathlib.Path] = <[unset; default: '.']>[0m
[90mlog.wandb.version: typing.Optional[str] = <[unset; default: None]>[0m
[90mlog.wandb.offline: bool = <[unset; default: False]>[0m
log.wandb.dir: typing.Union[str, pathlib.Path, NoneType] = '/root/work/run/fri-ootb/ootb-h100x1-lora_squad/wandb'
[90mlog.wandb.id: typing.Optional[str] = <[unset; default: None]>[0m
[90mlog.wandb.anonymous: typing.Optional[bool] = <[unset; default: None]>[0m
log.wandb.project: typing.Optional[str] = 'tut-nemo-sft'
[90mlog.wandb.log_model: typing.Union[typing.Literal['all'], bool] = <[unset; default: False]>[0m
[90mlog.wandb.experiment: typing.Union[ForwardRef('Run'), ForwardRef('RunDisabled'), NoneType] = <[unset; default: None]>[0m
[90mlog.wandb.prefix: str = <[unset; default: '']>[0m
[90mlog.wandb.checkpoint_name: typing.Optional[str] = <[unset; default: None]>[0m
[90mlog.wandb.add_file_policy: typing.Literal['mutable', 'immutable'] = <[unset; default: 'mutable']>[0m
log.wandb.config: Any = {}
log.wandb.entity: Any = 'vchua'
[90mlog.extra_loggers: typing.List[lightning.pytorch.loggers.logger.Logger] = <[unset; default: <factory>]>[0m
resume.restore_config.path: str = 'nemo://meta-llama/Meta-Llama-3-8B'
[90mresume.restore_config.load_model_state: bool = <[unset; default: True]>[0m
[90mresume.restore_config.load_optim_state: bool = <[unset; default: False]>[0m
[90mresume.restore_config.load_artifacts: bool = <[unset; default: True]>[0m
[90mresume.resume_from_directory: typing.Optional[str] = <[unset; default: None]>[0m
[90mresume.resume_from_path: typing.Optional[str] = <[unset; default: None]>[0m
[90mresume.resume_if_exists: bool = <[unset; default: False]>[0m
[90mresume.resume_past_end: bool = <[unset; default: False]>[0m
[90mresume.resume_ignore_no_checkpoint: bool = <[unset; default: False]>[0m
optim.config.optimizer: str = 'adam'
optim.config.lr: typing.Optional[float] = 0.0001
[90moptim.config.min_lr: typing.Optional[float] = <[unset; default: None]>[0m
[90moptim.config.decoupled_lr: typing.Optional[float] = <[unset; default: None]>[0m
[90moptim.config.decoupled_min_lr: typing.Optional[float] = <[unset; default: None]>[0m
optim.config.weight_decay: float = 0.1
[90moptim.config.fp8_recipe: typing.Optional[str] = <[unset; default: None]>[0m
optim.config.fp16: bool = False
optim.config.bf16: bool = True
[90moptim.config.reuse_grad_buf_for_mxfp8_param_ag: bool = <[unset; default: False]>[0m
[90moptim.config.params_dtype: dtype = <[unset; default: torch.float32]>[0m
[90moptim.config.use_precision_aware_optimizer: bool = <[unset; default: False]>[0m
[90moptim.config.store_param_remainders: bool = <[unset; default: True]>[0m
[90moptim.config.main_grads_dtype: dtype = <[unset; default: torch.float32]>[0m
[90moptim.config.main_params_dtype: dtype = <[unset; default: torch.float32]>[0m
[90moptim.config.exp_avg_dtype: dtype = <[unset; default: torch.float32]>[0m
[90moptim.config.exp_avg_sq_dtype: dtype = <[unset; default: torch.float32]>[0m
[90moptim.config.loss_scale: typing.Optional[float] = <[unset; default: None]>[0m
[90moptim.config.initial_loss_scale: float = <[unset; default: 4294967296]>[0m
[90moptim.config.min_loss_scale: float = <[unset; default: 1.0]>[0m
[90moptim.config.loss_scale_window: float = <[unset; default: 1000]>[0m
[90moptim.config.hysteresis: int = <[unset; default: 2]>[0m
optim.config.adam_beta1: float = 0.9
optim.config.adam_beta2: float = 0.98
optim.config.adam_eps: float = 1e-05
[90moptim.config.sgd_momentum: float = <[unset; default: 0.9]>[0m
optim.config.use_distributed_optimizer: bool = False
[90moptim.config.overlap_param_gather_with_optimizer_step: bool = <[unset; default: False]>[0m
[90moptim.config.optimizer_cpu_offload: bool = <[unset; default: False]>[0m
[90moptim.config.optimizer_offload_fraction: float = <[unset; default: 0.0]>[0m
[90moptim.config.use_torch_optimizer_for_cpu_offload: bool = <[unset; default: False]>[0m
[90moptim.config.overlap_cpu_optimizer_d2h_h2d: bool = <[unset; default: False]>[0m
[90moptim.config.pin_cpu_grads: bool = <[unset; default: True]>[0m
[90moptim.config.pin_cpu_params: bool = <[unset; default: True]>[0m
optim.config.clip_grad: float = 1.0
[90moptim.config.log_num_zeros_in_grad: bool = <[unset; default: False]>[0m
[90moptim.config.barrier_with_L1_time: bool = <[unset; default: False]>[0m
[90moptim.config.timers: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90moptim.config.config_logger_dir: str = <[unset; default: '']>[0m
[90moptim.lr_scheduler.max_steps: int = <[unset; default: 10]>[0m
optim.lr_scheduler.warmup_steps: int = 50
optim.lr_scheduler.constant_steps: int = 0
optim.lr_scheduler.min_lr: float = 0
[90moptim.lr_scheduler.interval: str = <[unset; default: 'step']>[0m
[90moptim.lr_scheduler.frequency: int = <[unset; default: 1]>[0m
[90moptim.lr_scheduler.monitor: str = <[unset; default: 'val_loss']>[0m
[90moptim.no_weight_decay_cond: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90moptim.scale_lr_cond: typing.Optional[typing.Callable] = <[unset; default: None]>[0m
[90moptim.lr_mult: float = <[unset; default: 1.0]>[0m
[90mpeft.target_modules: typing.List[str] = <[unset; default: <factory>]>[0m
[90mpeft.exclude_modules: typing.List[str] = <[unset; default: <factory>]>[0m
[90mpeft.canonical_mapping: typing.Dict[str, typing.Set] = <[unset; default: <factory>]>[0m
peft.dim: int = 8
peft.alpha: int = 16
[90mpeft.dropout: float = <[unset; default: 0.0]>[0m
[90mpeft.dropout_position: typing.Literal['pre', 'post'] = <[unset; default: 'pre']>[0m
[90mpeft.lora_A_init_method: str = <[unset; default: 'xavier']>[0m
[90mpeft.lora_B_init_method: str = <[unset; default: 'zero']>[0m
[90mpeft.a2a_experimental: bool = <[unset; default: False]>[0m
[90mpeft.lora_dtype: dtype = <[unset; default: None]>[0m
[90mpeft.dropout_recompute: bool = <[unset; default: False]>[0m
tokenizer: typing.Optional[typing.Any] = 'model'
