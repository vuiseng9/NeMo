{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "vs-py",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}",
            "program": "${file}",
            "args": []
        },
        {
            "name": "eval mmlu",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                // "CUBLASLT_LOG_LEVEL": "2",
                // "CUBLASLT_LOG_FILE": "./log.cublaslt",
                // "CUBLASLT_LOG_MASK": "31",
            },
            "preLaunchTask": "delete-cublaslt-log",
            "cwd": "${workspaceFolder}/scripts/nvfp4fw_mxfp8bw",
            "program": "in_mem_eval_mmlu.py",
            "args": [
                "--nemo_ckpt", 
                "/root/work/nemo/models/Qwen/Qwen3-8B",
                // "/root/work/run/mon-nv4f_mx8b/experiments/Qwen3-8B/Qwen3-8B_1761014654/02_Qwen3-8B_sft_f8_nv4f_mx8b/default/2025-10-21_02-44-29/checkpoints/model_name=0--val_loss=0.00-step=199-consumed_samples=102400.0-last",
                "--precision", 
                // "mxfp8", 
                "nv4f_mx8b"
            ]
        },
        {
            "name": "dev llama8b perf dev",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "NEMORUN_HOME": "/root/work/run/perf-dev",
                "NEMO_HOME": "/root/work/nemo",
                "WANDB_API_KEY": "",
                "HF_TOKEN": "",
                // "CUBLASLT_LOG_MASK": "31",
                // "CUDA_LAUNCH_BLOCKING": "1",
            },
            "cwd": "${workspaceFolder}",
            "module": "scripts.performance.llm.pretrain_llama3_8b",
            "args": [
                "-g", "b200", 
                "--max_steps", "15", 
                "--compute_dtype", "fp8", 
                "--fp8_recipe", 
                // "ds", 
                "mxfp8",
                // "nv4f_mx8b", 
                "-ng", "1", "-gn", "1", 
                "-tp", "1", "-pp", "1", "-vp", "1", "-cp", "1", "-ep", "1", "-et", "1", "-gb", "1", "-mb", "1",
            //    "--dryrun"
            ]
        },
        {
            "name": "dev sft qwen3 8b fp8 lora",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "NEMORUN_HOME": "/root/work/run/eval-dev",
                "NEMO_HOME": "/root/work/nemo",
                "WANDB_API_KEY": "",
                "HF_TOKEN": "",
            },
            "cwd": "${workspaceFolder}",
            "module": "scripts.nvfp4fw_mxfp8bw.finetune_qwen3_8b",
            "args": [
               "-f", "lora",
               "-g", 
               "h100",
            //    "b200", 
               "--num_gpus", "1", "--gpus_per_node", "1",
               "--max_steps", "200", 
               "-mb", "1", "-gb", "2", "-tp", "1", "-pp", "1", "-cp", "1", "-vp", "1", "-ep", "1", "-et", "1", 
            //    "-cg", "1",
               "--compute_dtype", "fp8", "--fp8_recipe", 
               "ds",
               "-wdp", "tut-nemo-sft",
            //    "mxfp8",
            //    "nv4f_mx8b",
            //    "--dryrun"
            ]
        },
        {
            "name": "ootb h100x1 fp8 nemo lora",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "NEMORUN_HOME": "/root/work/run/dev-perf-sft",
                "NEMO_HOME": "/root/work/nemo",
            },
            "cwd": "${workspaceFolder}",
            "program": "/usr/local/bin/nemo",
            "args": [
                "llm", "finetune", "--factory", "llama3_8b",
                "trainer.plugins=nemo.collections.llm.recipes.precision.mixed_precision.bf16_with_fp8_mixed",
                "trainer.accelerator=gpu", "trainer.num_nodes=1", "trainer.devices=1",
                "trainer.max_steps=50", "trainer.val_check_interval=10",
                "log.log_dir=/root/work/run/fri-noon-ootb/",
                // "log.ckpt.every_n_train_steps=10 log.ckpt.save_top_k=1",
                // "log.wandb.project=tut-nemo-sft",
                // "log.wandb.name=$run_label",
                // "log.wandb.entity=vchua",
                // "log.wandb.dir=$logdir/wandb",
            ]
        },
        {
            "name": "bench perf",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "/opt/NeMo/scripts/performance/llm",
            "module": "scripts.performance.llm.pretrain_llama3_8b",
            "args": [
                // "-h",
                "-g", "b200", "--compute_dtype", "fp8", "--dryrun"
                // "--fp8_recipe", "mxfp8",
                // "--gpus_per_node", "1", "--num_gpus", "1",
                // "-mb", "2",
                // "-gb", "8",
                // "-tp", "1",
                // "-pp", "1",
                // "-cp", "1",
                // "-vp", "1",
                // "-ep", "1",
                // "--cuda_graphs", "1",
                // "--max_steps", "10",
            ]
        },
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        }
    ]
}

setup need
graphviz (take this away)
hf login has some issue, token doesnt work


    with run.Experiment(exp_name) as exp:
        if not (Path(os.getenv("NEMO_HOME",""))/"models"/ HF_MODEL_URI).exists():
            if os.getenv("HUGGING_FACE_HUB_TOKEN") is None:
                raise ValueError("Model not found, need to download from hf and convert nemo format, pls set/export HUGGING_FACE_HUB_TOKEN")
            

git clone https://github.com/NVIDIA-NeMo/Run sample_nemo_run

export WANDB_API_KEY=
export HF_TOKEN=

export NEMO_HOME=/root/work/nemo
export NEMORUN_HOME=/root/work/run/eval-dev

runtype=lora
device=h100

python -m scripts.nvfp4fw_mxfp8bw.finetune_qwen3_8b -g $device \
    -f $runtype \
    --num_gpus 1 --gpus_per_node 2 \
    -wdp tut-nemo-sft \
    --compute_dtype fp8 --fp8_recipe ds \
    -mb 1 -gb 8 -tp 1 -pp 1 -cp 1 -vp 1 -ep 1 -et 1
    


// ----------------------------------------------------------------------
python -m scripts.nvfp4fw_mxfp8bw.finetune_qwen3_8b -g b200 \
    -f sft \
    -wdp tut-nemo-sft \
    -mb 1 -gb 512 -tp 1 -pp 1 -cp 1 -vp 1 -ep 1 -et 1 \
    --compute_dtype fp8 --fp8_recipe mxfp8

    --compute_dtype fp8 --fp8_recipe ds
    --compute_dtype bf16
    // --num_gpus 1 --gpus_per_node 8 \
// ----------------------------------------------------------------------





    python -m scripts.nvfp4fw_mxfp8bw.finetune_qwen3_8b -g b200 \
    -f sft \
    --num_gpus 1 --gpus_per_node 1 \
    -wdp tut-nemo-sft \
    -mb 1 -gb 8 -tp 1 -pp 1 -cp 1 -vp 1 -ep 1 -et 1 \
    --compute_dtype fp8 --fp8_recipe mxfp8
// ----------------------------------------------------------------------


export NEMO_HOME=/root/work/nemo
run_label=ootb-h100x1-lora_squad
logdir=/root/work/run/fri-ootb/$run_label

nemo llm finetune --factory llama3_8b \
    trainer.plugins=nemo.collections.llm.recipes.precision.mixed_precision.bf16_with_fp8_mixed \
    trainer.accelerator=gpu trainer.num_nodes=1 trainer.devices=1 \
    trainer.max_steps=50 trainer.val_check_interval=10 \
    log.log_dir=$logdir \
    log.ckpt.every_n_train_steps=10 log.ckpt.save_top_k=1 \
    log.wandb.project=tut-nemo-sft \
    log.wandb.name=$run_label \
    log.wandb.entity=vchua \
    log.wandb.dir=$logdir/wandb

